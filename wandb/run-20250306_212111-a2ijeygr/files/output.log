Number of response logs 24000
[Epoch 1]
Training: 100%|███████████████████████████████████████████████████| 150/150 [00:02<00:00, 68.82it/s]
Average loss: 1.5379150716463725
Evaluating: 100%|██████████████████████████████████████████████████| 38/38 [00:00<00:00, 197.98it/s]
Traceback (most recent call last):
  File "d:\Git\Over-estimate\run.py", line 76, in <module>
    sys.exit(main(config_dict))
             ^^^^^^^^^^^^^^^^^
  File "d:\Git\Over-estimate\run.py", line 72, in main
    ncdm.train(datahub, "train", "test", valid_metrics=validate_metrics, batch_size=config['batch_size'],epoch=config['epoch'], weight_decay=0, lr=4e-3)
  File "d:\Git\Over-estimate\inscd\models\static\neural\ncdm.py", line 59, in train
    self._train(datahub=datahub, set_type=set_type,
  File "d:\Git\Over-estimate\inscd\_base.py", line 52, in _train
    self.score(datahub, valid_set_type, valid_metrics, **kwargs)
  File "d:\Git\Over-estimate\inscd\models\static\neural\ncdm.py", line 69, in score
    return self._score(datahub=datahub, set_type=set_type, metrics=metrics, batch_size=batch_size)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Git\Over-estimate\inscd\_listener.py", line 33, in wrapper
    result = self.__format(func(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^
  File "d:\Git\Over-estimate\inscd\_base.py", line 67, in _score
    return ruler(self, datahub, set_type, pred_r, metrics)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Git\Over-estimate\inscd\_ruler.py", line 159, in __call__
    results[metric] = self.__method_map[metric](true_r, pred_r)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\Git\Over-estimate\inscd\_ruler.py", line 35, in area_under_curve
    return roc_auc_score(true_r, pred_r,multi_class="ovr")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\admin.DESKTOP-7USASVH\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\admin.DESKTOP-7USASVH\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\admin.DESKTOP-7USASVH\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes
